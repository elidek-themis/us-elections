{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "from src.definitions import Experiment, states, results_2020\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AwqConfig, GenerationConfig\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectionMessage():\n",
    "    \n",
    "    def __init__(self, chat: Union[str, List[Dict]]) -> None:\n",
    "        self.chat = chat\n",
    "    \n",
    "    def format(self, state) -> Union[str, List[Dict]]:\n",
    "        \n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat.format(state=state)\n",
    "        \n",
    "        elif isinstance(self.chat, List[Dict]):\n",
    "            chat = [dict(message) for message in self.chat]\n",
    "            for message in chat:\n",
    "                if \"{state}\" in message[\"content\"]:\n",
    "                    message[\"content\"] = message[\"content\"].format(state=state)\n",
    "                    return chat\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer:AutoTokenizer, message: Union[str, List[Dict]]) -> torch.Tensor:\n",
    "    if isinstance(message, str):\n",
    "        return tokenizer.encode(message, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    elif isinstance(message, List[Dict]):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            conversation=message,\n",
    "            continue_final_message=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "def continuation_loss(\n",
    "    model:AutoModelForCausalLM,\n",
    "    tokenizer:AutoTokenizer,\n",
    "    context: Union[str, List[Dict]],\n",
    "    cont:str\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    context_encodings = tokenize(tokenizer=tokenizer, message=context)\n",
    "    cont_encodings = tokenizer.encode(cont, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = torch.cat((context_encodings, cont_encodings), dim=1).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits.permute(0, 2, 1) # vocab dimension last\n",
    "    logits = logits[:, :, :-1]\n",
    "\n",
    "    input_ids[:, :-cont_encodings.size(1)] = -100 # makes context ignored by loss function\n",
    "    input_ids = input_ids[:, 1:] # next-token-prediction => shift tokens\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")(logits, input_ids)\n",
    "    \n",
    "    return loss.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model_id = \"google/gemma-2-9b-it\"\n",
    "# model_id = \"tiiuae/falcon-mamba-7b\"\n",
    "model_id = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "# model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# model_id = \"facebook/opt-125m\"\n",
    "\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\"\n",
    "# quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=512, # Note: Update this as per your use-case\n",
    "#     do_fuse=True,\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    # quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_id = 1\n",
    "settings = Experiment.settings[setting_id]\n",
    "\n",
    "message = ElectionMessage(chat=settings[\"message\"])\n",
    "choices = settings[\"choices\"]\n",
    "pbar = tqdm(states)\n",
    "results = {}\n",
    "\n",
    "for state in pbar:\n",
    "    pbar.set_description(state)\n",
    "    results[state] = {}\n",
    "    context = message.format(state=state)\n",
    "    for choice in choices:\n",
    "        cont = \" \" + choice\n",
    "        negative_log_likelihood = continuation_loss(model=model,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    context=context,\n",
    "                                                    cont=cont\n",
    "                                                    )\n",
    "        results[state][choice] = negative_log_likelihood.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print last tokenization encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_enc = tokenize(tokenizer=tokenizer, message=context).tolist()[0]\n",
    "print (cont_enc)\n",
    "print(tokenizer.decode(cont_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_conts = int(len(choices) / 2)\n",
    "columns = [\"*\", \"* party\", \"* candidate\", \"* nominee\"][:num_conts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "nll_df.loc[\"US\"] = nll_df.mean()\n",
    "democratic_df = nll_df.iloc[:, :num_conts]\n",
    "republican_df = nll_df.iloc[:, num_conts:]\n",
    "objs = objs=(democratic_df, republican_df)\n",
    "nll_df = pd.concat(objs=objs, keys=(\"Democratic\", \"Republican\"), axis=1)\n",
    "# nll_df.to_excel(\"results/nlls.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = democratic_df.values - republican_df.values\n",
    "diff = pd.DataFrame(index=nll_df.index, data=data, columns=columns)\n",
    "diff[\"sum\"] = diff.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_diff = pd.DataFrame(index=nll_df.index, columns=columns)\n",
    "for i, col in enumerate(columns):    \n",
    "    exp_df = nll_df.iloc[:, [i, num_conts+i]].map(lambda x: exp(x))\n",
    "\n",
    "    blue_exp = exp_df.iloc[:, 0]\n",
    "    red_exp = exp_df.iloc[:, 1]\n",
    "    \n",
    "    numerator = blue_exp.sub(red_exp)\n",
    "    denumerator = blue_exp.add(red_exp)\n",
    "\n",
    "    exp_diff[col] = -numerator.div(denumerator)\n",
    "\n",
    "exp_diff[\"avg\"] = exp_diff.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2020 = pd.Series(results_2020)\n",
    "elections_map = results_2020.apply(lambda x: 0 if x > 0 else 1)\n",
    "\n",
    "agreement = exp_diff.drop(\"US\").map(lambda x: 0 if x < 0 else 1)\n",
    "agreement = agreement.apply(lambda x: x==elections_map).map(lambda x: 0 if x else 1)\n",
    "\n",
    "mean = agreement.mean()\n",
    "agreement.loc[\"US\"] = [\"Average agreement\"] + [\"\"] * len(columns)\n",
    "agreement.loc[\" \"] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute pct difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_dif_ag = exp_diff.drop(\"US\").apply(lambda x: x.sub(results_2020)).abs()\n",
    "abs_dif_disag = exp_diff.drop(\"US\").apply(lambda x: x.add(results_2020)).abs()\n",
    "\n",
    "abs_pct_diff = pd.DataFrame(index=results_2020.index, columns=exp_diff.columns)\n",
    "\n",
    "ag_idx = (agreement == 1)\n",
    "abs_pct_diff[ag_idx] = abs_dif_ag[ag_idx]\n",
    "\n",
    "disag_idx = (agreement == 0)\n",
    "abs_pct_diff[disag_idx] = abs_dif_disag[disag_idx]\n",
    "\n",
    "mean = abs_pct_diff.mean()\n",
    "abs_pct_diff.loc[\"US\"] = [\"Average absolute % diff\"] + [\"\"] * len(columns)\n",
    "abs_pct_diff.loc[\" \"] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation and .xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color(val, cond):\n",
    "    color = \"#a4c2f4\" if cond(val) else \"#ea9999\"\n",
    "    return \"background-color: %s\" % color\n",
    "\n",
    "def _agree_color(val):\n",
    "    if val == 0:\n",
    "        return \"background-color: #e06666\"\n",
    "\n",
    "def bold_fn(x, fn:callable) -> List[str]:\n",
    "    condition = lambda v: v == fn(x) or type(v) == str\n",
    "    return ['font-weight: bold' if condition(v) else '' for v in x]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Colormap():\n",
    "    \n",
    "    nll_bar = {\n",
    "        \"cmap\": \"RdYlGn_r\",\n",
    "        \"subset\": pd.IndexSlice[\"Negative Log Likelihood\"],\n",
    "    }\n",
    "    \n",
    "    diff = {\n",
    "        \"func\": lambda x: _color(x, cond=lambda x: x<0),\n",
    "        \"subset\": pd.IndexSlice[states + [\"US\"], \"Differences\"]\n",
    "    }\n",
    "    \n",
    "    pct = {\n",
    "        \"func\": lambda x: _color(x, cond=lambda x: x>0),\n",
    "        \"subset\": pd.IndexSlice[states + [\"US\"], \"Percentages\"]\n",
    "    }\n",
    "    \n",
    "    past_results = {\n",
    "        \"func\": lambda x: _color(x, cond=lambda x: x>0),\n",
    "        \"subset\": pd.IndexSlice[states, \"2020\"]\n",
    "    }\n",
    "    \n",
    "    agree = {\n",
    "        \"func\": _agree_color,\n",
    "        \"subset\": pd.IndexSlice[states, \"Agreement\"] \n",
    "    }\n",
    "    \n",
    "    nll_gradient = {\n",
    "        \"cmap\": \"RdYlGn_r\",\n",
    "        \"subset\": pd.IndexSlice[states + [\"US\"], \"Negative Log Likelihood\"]\n",
    "    }\n",
    "    \n",
    "    abs_pct_gradient = {\n",
    "        \"cmap\": \"Greens_r\",\n",
    "        \"subset\": pd.IndexSlice[states, \"Absolute percentage difference\"],\n",
    "        \"vmin\": 0,\n",
    "        \"vmax\": 1,\n",
    "        \"text_color_threshold\": 0\n",
    "    }\n",
    "      \n",
    "    def color(color: str, col: Tuple) -> Dict:\n",
    "        return {\n",
    "            \"func\": lambda _: f\"background-color: {color}\",\n",
    "            \"subset\": pd.IndexSlice[col]\n",
    "        }    \n",
    "    \n",
    "objs = {\n",
    "    \"Negative Log Likelihood\": nll_df.droplevel(0, axis=1),\n",
    "    \"Differences\": diff,\n",
    "    \"Percentages\": exp_diff,\n",
    "    \"2020\": results_2020.to_frame(name=\"\"),\n",
    "    \"Agreement\": agreement,\n",
    "    \"Absolute percentage difference\": abs_pct_diff\n",
    "    }\n",
    "\n",
    "stats = pd.concat(objs=objs.values(), keys=objs.keys(), axis=1)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "f_name = f\"{setting_id}. {os.path.basename(model_id).lower()}\"\n",
    "\n",
    "stats_styled = (\n",
    "    stats.style.map(**Colormap.diff)\n",
    "    .map(**Colormap.pct)\n",
    "    .map(**Colormap.past_results)\n",
    "    .map(**Colormap.agree)\n",
    "    .background_gradient(**Colormap.abs_pct_gradient)\n",
    "    .map(**Colormap.color(color=\"#d5a6bd\", col=([\"US\", \" \"], \"Agreement\")))\n",
    "    .map(**Colormap.color(color=\"#ffe599\", col=([\"US\", \" \"], \"Absolute percentage difference\")))\n",
    "    .apply(lambda x: bold_fn(x, fn=max),\n",
    "           subset=pd.IndexSlice[[\"US\", \" \"], \"Agreement\"],\n",
    "           axis=1\n",
    "           )\n",
    "    .apply(lambda x: bold_fn(x, fn=min),\n",
    "           subset=pd.IndexSlice[[\"US\", \" \"], \"Absolute percentage difference\"],\n",
    "           axis=1\n",
    "           )\n",
    "    .bar(**Colormap.nll_bar)\n",
    "    )\n",
    "\n",
    "stats_styled.to_excel(os.path.join(\"results\", f\"{f_name}.xlsx\"), engine=\"xlsxwriter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elections",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
