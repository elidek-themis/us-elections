{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch_karanikolopoulos/miniconda3/envs/elections/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "from src.definitions import Experiment, states, populations, RESULTS_PATH\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AwqConfig, GenerationConfig\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectionMessage():\n",
    "    \n",
    "    def __init__(self, chat: Union[str, List]) -> None:\n",
    "        self.chat = chat\n",
    "    \n",
    "    def format(self, persona) -> Union[str, List]:\n",
    "        \n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat.format(persona=persona)\n",
    "        \n",
    "        elif isinstance(self.chat, List):\n",
    "            chat = [dict(message) for message in self.chat]\n",
    "            for message in chat:\n",
    "                if \"{persona}\" in message[\"content\"]:\n",
    "                    message[\"content\"] = message[\"content\"].format(persona=persona)\n",
    "            return chat\n",
    "    \n",
    "    def remove_residency(self) -> Union[str, List]:        \n",
    "        def _rm(text: str) -> str:\n",
    "            prefix, _, suffix = text.split(\", \")\n",
    "            return ', '.join((prefix, suffix))\n",
    "        \n",
    "        if isinstance(self.chat, str):\n",
    "            return _rm(text=self.chat)\n",
    "        elif isinstance(self.chat, List):\n",
    "            chat = [dict(message) for message in self.chat]\n",
    "            for message in chat:\n",
    "                message[\"content\"] = _rm(text=message[\"content\"])     \n",
    "            return chat\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat\n",
    "        elif isinstance(self.chat, List):\n",
    "            \n",
    "            r = (f\"User: {self.chat[0]['content']}\",\n",
    "                 f\"Assistant: {self.chat[1]['content']}\")\n",
    "            \n",
    "            return \"\\n\".join(r)\n",
    "\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": \"As a U.S. citizen, resident of {persona}, who will you vote for in the 2024 U.S. presidential election?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for the candidate of the\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer:AutoTokenizer, message: Union[str, List]) -> torch.Tensor:\n",
    "    if isinstance(message, str): # let the tokenizer decide for special tokens\n",
    "        return tokenizer(message, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    elif isinstance(message, List):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            conversation=message,\n",
    "            continue_final_message=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "def continuation_loss(\n",
    "    model:AutoModelForCausalLM,\n",
    "    tokenizer:AutoTokenizer,\n",
    "    context: Union[str, List],\n",
    "    cont:str\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    context_encodings = tokenize(tokenizer=tokenizer, message=context)\n",
    "    cont_encodings = tokenizer.encode(cont, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = torch.cat((context_encodings, cont_encodings), dim=1).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits.permute(0, 2, 1) # vocab dimension last\n",
    "    logits = logits[:, :, :-1]\n",
    "\n",
    "    input_ids[:, :-cont_encodings.size(1)] = -100 # makes context ignored by loss function\n",
    "    input_ids = input_ids[:, 1:] # next-token-prediction => shift tokens\n",
    "\n",
    "    \n",
    "    nll_losses = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, input_ids)\n",
    "    \n",
    "    return nll_losses.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experiment():\n",
    "    \n",
    "    choices_1 = [\n",
    "        \"Democratic party\",\n",
    "        \"Democratic candidate\",\n",
    "        \"Democratic nominee\",\n",
    "        \"Republican party\",\n",
    "        \"Republican candidate\",\n",
    "        \"Republican nominee\"  \n",
    "    ]\n",
    "    \n",
    "    choices_2 = [\n",
    "        \"Democrats\",\n",
    "        \"Democratic party\",\n",
    "        \"Republicans\",\n",
    "        \"Republican party\"\n",
    "    ]\n",
    "    \n",
    "    choices_3 = [\n",
    "        \"Joe Biden\",\n",
    "        \"Biden\",\n",
    "        \"Donald Trump\",\n",
    "        \"Trump\",\n",
    "    ]\n",
    "    \n",
    "    settings = {\n",
    "        1: {\n",
    "            \"message\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for the\",\n",
    "            \"choices\": choices_1,\n",
    "            \"num_conts\": 3,\n",
    "            \"columns\": [\"* party\", \"* candidate\", \"* nominee\"]\n",
    "        },\n",
    "        \"1-chat\": {\n",
    "            \"message\": [\n",
    "                {\"role\": \"user\", \"content\": \"As a U.S. citizen, resident of {persona}, who will you vote for in the 2024 U.S. presidential election?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for the\"}\n",
    "                ],\n",
    "            \"choices\": choices_1,\n",
    "            \"num_conts\": 3,\n",
    "            \"columns\": [\"* party\", \"* candidate\", \"* nominee\"]\n",
    "        },\n",
    "        2: {\n",
    "            \"message\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for the candidate of the\",\n",
    "            \"choices\": choices_2,\n",
    "            \"num_conts\": 2,\n",
    "            \"columns\": [\"*\", \"* party\"]\n",
    "        },\n",
    "        \"2-chat\": {\n",
    "            \"message\": [\n",
    "                {\"role\": \"user\", \"content\": \"As a U.S. citizen, resident of {persona}, who will you vote for in the 2024 U.S. presidential election?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for the candidate of the\"}\n",
    "                ],\n",
    "            \"choices\": choices_2,\n",
    "            \"num_conts\": 2,\n",
    "            \"columns\": [\"*\", \"* party\"]\n",
    "        },\n",
    "        3: {\n",
    "            \"message\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for\",\n",
    "            \"choices\": choices_3,\n",
    "            \"num_conts\": 2,\n",
    "            \"columns\": [\"fullname\", \"lastname\"]\n",
    "        },\n",
    "        \"3-chat\": {\n",
    "            \"message\": [\n",
    "                {\"role\": \"user\", \"content\": \"As a U.S. citizen, resident of {persona}, who will you vote for in the 2024 U.S. presidential election?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for\"}\n",
    "                ],\n",
    "            \"choices\": choices_3,\n",
    "            \"num_conts\": 2,\n",
    "            \"columns\": [\"fullname\", \"lastname\"]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_df(results: Dict, num_conts: int) -> pd.DataFrame:\n",
    "    df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "    blue_idx = df.iloc[:, :num_conts].columns\n",
    "    red_idx = df.iloc[:, num_conts:].columns\n",
    "    objs = (df[blue_idx], df[red_idx])\n",
    "    \n",
    "    return pd.concat(objs=objs, keys=(\"Democratic\", \"Republican\"), axis=1)\n",
    "\n",
    "def get_prob_df(nll_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Probabilities and normalized probabilities for every continuation\"\"\"\n",
    "    \n",
    "    prob_df = (-nll_df).map(lambda x: exp(x)) # exp(LogLikelihood)\n",
    "    # democratic probability sum\n",
    "    prob_df[\"Democratic\", \"D_sum\"] = prob_df[\"Democratic\"].sum(axis=1)\n",
    "    # republican probability sum\n",
    "    prob_df[\"Republican\", \"R_sum\"] = prob_df[\"Republican\"].sum(axis=1)\n",
    "    prob_df = prob_df[[\"Democratic\", \"Republican\"]]\n",
    "\n",
    "    norm_prob_df = prob_df.copy()\n",
    "    no_cols = int(len(prob_df.columns)/2)\n",
    "    for i in range(no_cols):   \n",
    "        probs = prob_df.iloc[:, [i, no_cols+i]]\n",
    "        # P(D) / sum(P(D) + P(R))\n",
    "        norm_prob_df.iloc[:, i] = norm_prob_df.iloc[:, i].div(probs.sum(axis=1))\n",
    "        # P(R) / sum(P(D) + P(R))\n",
    "        norm_prob_df.iloc[:, no_cols+i] = norm_prob_df.iloc[:, no_cols+i].div(probs.sum(axis=1))\n",
    "        \n",
    "    return prob_df, norm_prob_df\n",
    "\n",
    "def get_differences(norm_prob_df: pd.DataFrame, columns: List) -> pd.DataFrame:\n",
    "    cols = columns + [\"* sum\"]\n",
    "    data = norm_prob_df[\"Democratic\"].values - norm_prob_df[\"Republican\"].values\n",
    "    \n",
    "    return pd.DataFrame(index=norm_prob_df.index, data=data, columns=cols)\n",
    "\n",
    "def get_exp_differences(df: pd.DataFrame, columns: List, num_conts: int) -> pd.DataFrame:\n",
    "    exp_diff = pd.DataFrame(index=df.index, columns=columns)\n",
    "    for i, col in enumerate(columns):    \n",
    "        exp_df = df.iloc[:, [i, num_conts+i]].map(lambda x: exp(x))\n",
    "\n",
    "        blue_exp = exp_df.iloc[:, 0]\n",
    "        red_exp = exp_df.iloc[:, 1]\n",
    "        \n",
    "        numerator = blue_exp.sub(red_exp)\n",
    "        denominator = blue_exp.add(red_exp)\n",
    "\n",
    "        exp_diff[col] = -numerator.div(denominator)\n",
    "\n",
    "    exp_diff[\"avg\"] = exp_diff.mean(axis=1)\n",
    "    \n",
    "    return exp_diff\n",
    "\n",
    "def get_voting(voting_path: str) -> pd.DataFrame:\n",
    "    voting = pd.read_excel(voting_path, index_col=0, usecols=[\"state\", \"red_pct\", \"blue_pct\"])\n",
    "    voting = voting.apply(lambda x: x.div(voting.sum(axis=1)))\n",
    "    voting[\"pct_diff\"] = voting.blue_pct - voting.red_pct\n",
    "    \n",
    "    return voting\n",
    "\n",
    "\n",
    "def get_agreement(voting: pd.DataFrame, diff: pd.DataFrame, columns: List) -> pd.DataFrame:\n",
    "    elections_map = voting.pct_diff.apply(lambda x: 0 if x < 0 else 1)\n",
    "\n",
    "    agreement = diff.drop(\"US\").map(lambda x: 0 if x > 0 else 1)\n",
    "    agreement = agreement.apply(lambda x: x==elections_map).map(lambda x: 0 if x else 1)\n",
    "\n",
    "    # stats rows\n",
    "    agreement.loc[\"avg\"] = agreement.mean()\n",
    "\n",
    "    return agreement\n",
    "\n",
    "def get_ext_diff(diff: pd.DataFrame, agreement:pd.DataFrame, weights: Dict, columns:List) -> pd.DataFrame:\n",
    "    weights = pd.Series(populations)\n",
    "    ext_diff = diff.copy()\n",
    "\n",
    "    ext_diff.loc[\"avg\"] = ext_diff.loc[states].mean(axis=0)\n",
    "    ext_diff.loc[\"weighted avg\"] = (ext_diff.loc[states].mul(weights, axis=\"index\") / weights.sum()).mean()\n",
    "    \n",
    "    maj_counts = ext_diff.loc[states].map(lambda x: x>0)\n",
    "    dem_counts = maj_counts[maj_counts == True].sum()\n",
    "    rep_counts = maj_counts[maj_counts == False].sum()\n",
    "    ext_diff.loc[\"dem/rep\"] = [f\"{d}/{r}\" for d,r in zip(dem_counts, rep_counts)]\n",
    "    \n",
    "    ext_diff.loc[\" \"] = [\"Average agreement\"] + [\"\"] * len(columns)\n",
    "    ext_diff.loc[\"  \"] = agreement.loc[\"avg\"]\n",
    "    \n",
    "    return ext_diff\n",
    "\n",
    "\n",
    "def get_abs_pct_difference(\n",
    "    voting: pd.DataFrame,\n",
    "    diff: pd.DataFrame,\n",
    "    agreement: pd.DataFrame,\n",
    "    columns: List\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    abs_dif_ag = diff.drop(\"US\").apply(lambda x: x.sub(voting.pct_diff)).abs()\n",
    "    abs_dif_disag = diff.drop(\"US\").apply(lambda x: x.add(voting.pct_diff)).abs()\n",
    "\n",
    "    abs_pct_diff = pd.DataFrame(index=voting.index, columns=diff.columns)\n",
    "\n",
    "    abs_pct_diff[agreement == 1] = abs_dif_ag[agreement == 1]\n",
    "    abs_pct_diff[agreement == 0] = abs_dif_disag[agreement == 0]\n",
    "\n",
    "    # stats rows\n",
    "    mean = abs_pct_diff.mean()\n",
    "    abs_pct_diff.loc[\" \"] = [\"Average absolute difference\"] + [\"\"] * len(columns)\n",
    "    abs_pct_diff.loc[\"  \"] = mean\n",
    "    \n",
    "    return abs_pct_diff\n",
    "\n",
    "\n",
    "def get_relative_error(\n",
    "    norm_prob_df: pd.DataFrame,\n",
    "    voting: pd.DataFrame,\n",
    "    columns: List\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    blue_err = norm_prob_df.drop(\"US\")[\"Democratic\"].apply(lambda x: x.sub(voting.blue_pct).abs().div(voting.red_pct))\n",
    "    blue_err.columns = columns + [\"* sum\"]\n",
    "\n",
    "    red_err = norm_prob_df.drop(\"US\")[\"Republican\"].apply(lambda x: x.sub(voting.red_pct).abs().div(voting.blue_pct))\n",
    "    red_err.columns = columns + [\"* sum\"]\n",
    "\n",
    "    objs = (red_err.loc[voting.pct_diff < 0], blue_err.loc[voting.pct_diff > 0])\n",
    "    error_df = pd.concat(objs=objs).sort_index()\n",
    "\n",
    "    # stats rows\n",
    "    mean = error_df.mean()\n",
    "    error_df.loc[\" \"] = [\"Average relative error\"] + [\"\"] * len(columns)\n",
    "    error_df.loc[\"  \"] = mean\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "def get_voting_stats(\n",
    "    label:str,\n",
    "    voting_path: str,\n",
    "    norm_prob_df: pd.DataFrame,\n",
    "    diff: pd.DataFrame,\n",
    "    columns:List\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    voting = get_voting(voting_path=voting_path)\n",
    "    agreement = get_agreement(voting=voting, diff=diff, columns=columns)\n",
    "    \n",
    "    ext_diff = get_ext_diff(diff=diff, agreement=agreement, weights=populations, columns=columns)\n",
    "    abs_pct_diff = get_abs_pct_difference(voting=voting, diff=diff, agreement=agreement, columns=columns)\n",
    "    error_df = get_relative_error(norm_prob_df=norm_prob_df, voting=voting,columns=columns)\n",
    "    \n",
    "    stats = {\n",
    "        label: voting,\n",
    "        \"Predicted Differences (agreement)\": ext_diff,\n",
    "        \"Probability Absolute Difference\": abs_pct_diff,\n",
    "        \"Relative Error (winning party)\": error_df\n",
    "    }\n",
    "\n",
    "    return pd.concat(objs=stats.values(), keys=stats.keys(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# model_id = \"google/gemma-2-9b\"\n",
    "# model_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-v0.3\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_id = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "\n",
    "# model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# model_id = \"HuggingFaceH4/zephyr-7b-beta\" # based on mistral\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "# model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# model_id = \"tiiuae/falcon-11B\"\n",
    "# model_id = \"tiiuae/falcon-mamba-7b\"\n",
    "\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\"\n",
    "# quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=512, # Note: Update this as per your use-case\n",
    "#     do_fuse=True,\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    # quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.1,\n",
    "    top_k=100,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    # eos_token_id=model.config.eos_token_id,\n",
    "    # pad_token=model.config.pad_token_id,\n",
    ")\n",
    "\n",
    "user_pr = \"As a US citizen, in the 2024 US presidential election I will vote for the candidates of the\"\n",
    "# assis_resp = \"As a resident of Idaho, in the 2024 US presidential election I will vote for the\"\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"As a U.S. citizen, resident of {persona}, who will you vote for in the 2024 U.S. presidential election?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"As a U.S. citizen, resident of {persona}, in the 2024 US presidential election I will vote for\"},\n",
    "]\n",
    "\n",
    "# inputs = tokenizer(user_pr, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    conversation=chat,\n",
    "    continue_final_message=True,\n",
    "    return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(inputs, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "US: 100%|██████████| 52/52 [00:15<00:00,  3.43it/s]                 \n"
     ]
    }
   ],
   "source": [
    "setting_id = 3\n",
    "settings = Experiment.settings[setting_id]\n",
    "message = ElectionMessage(chat=settings[\"message\"])\n",
    "choices = settings[\"choices\"]\n",
    "num_conts = settings[\"num_conts\"]\n",
    "columns = settings[\"columns\"]\n",
    "\n",
    "pbar = tqdm(states + [\"US\"])\n",
    "results = {}\n",
    "\n",
    "for state in pbar:\n",
    "    pbar.set_description(state)\n",
    "    results[state] = {}\n",
    "    \n",
    "    if state == \"US\":\n",
    "        context = message.remove_residency()\n",
    "    else:\n",
    "        context = message.format(persona=state)\n",
    "    \n",
    "    for choice in choices:\n",
    "        cont = \" \" + choice\n",
    "        negative_log_likelihood = continuation_loss(model=model,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    context=context,\n",
    "                                                    cont=cont\n",
    "                                                    )\n",
    "        results[state][choice] = negative_log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_df = get_nll_df(results=results, num_conts=num_conts) # Negative Log Likelihoods\n",
    "prob_df, norm_prob_df = get_prob_df(nll_df=nll_df) # Probabilities\n",
    "diff = get_differences(norm_prob_df=norm_prob_df, columns=columns) # Probability Differences\n",
    "\n",
    "outputs = {\n",
    "    # \"Negative Log Likelihood\": nll_df.droplevel(0, axis=1),\n",
    "    # \"Probabilities\": prob_df.droplevel(0, axis=1),\n",
    "    \"Normalized Probabilities\": norm_prob_df.droplevel(0, axis=1),\n",
    "    \"Probability Differences\": diff\n",
    "}\n",
    "\n",
    "outputs = pd.concat(objs=outputs.values(), keys=outputs.keys(), axis=1)\n",
    "\n",
    "stats_20 = get_voting_stats(label=\"2020\",\n",
    "                            voting_path=\"../data/voting-2020.xlsx\",\n",
    "                            diff=diff,\n",
    "                            norm_prob_df=norm_prob_df,\n",
    "                            columns=columns)\n",
    "\n",
    "stats_24 = get_voting_stats(label=\"2024\",\n",
    "                            voting_path=\"../data/voting-2024.xlsx\",\n",
    "                            diff=diff,\n",
    "                            norm_prob_df=norm_prob_df,\n",
    "                            columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = f\"{setting_id}. {os.path.basename(model_id).lower()}\"\n",
    "with pd.ExcelWriter(\"test.xlsx\") as writer:\n",
    "    outputs.to_excel(writer, sheet_name=\"Outputs\")  \n",
    "    stats_20.to_excel(writer, sheet_name=\"2020\")\n",
    "    stats_24.to_excel(writer, sheet_name=\"2024\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': [{'role': 'user',\n",
       "   'content': 'As a U.S. citizen, resident of {persona}, who will you vote for in the 2024 U.S. presidential election?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'As a U.S. citizen, resident of {persona}, in the 2024 U.S. presidential election I will vote for'}],\n",
       " 'choices': ['Joe Biden', 'Biden', 'Donald Trump', 'Trump'],\n",
       " 'num_conts': 2,\n",
       " 'columns': ['fullname', 'lastname']}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = None\n",
    "base_models = [\n",
    "\"meta-llama/Llama-3.1-8B\",\n",
    "\"meta-llama/Llama-3.2-3B\",\n",
    "\"google/gemma-2-9b\",\n",
    "\"mistralai/Mistral-7B-v0.3\",\n",
    "\"tiiuae/falcon-7b\"\n",
    "]\n",
    "\n",
    "instruct_models = [\n",
    "\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "\"google/gemma-2-9b-it\",\n",
    "\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "\"tiiuae/falcon-7b-instruct\"\n",
    "]\n",
    "\n",
    "setting_id = \"3-chat\"\n",
    "settings = Experiment.settings[setting_id]\n",
    "message = ElectionMessage(chat=settings[\"message\"])\n",
    "choices = settings[\"choices\"]\n",
    "num_conts = settings[\"num_conts\"]\n",
    "columns = settings[\"columns\"]\n",
    "\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.95s/it]\n",
      "US: 100%|██████████| 52/52 [00:59<00:00,  1.15s/it]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.89s/it]\n",
      "US: 100%|██████████| 52/52 [00:42<00:00,  1.21it/s]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2-9b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]\n",
      "US: 100%|██████████| 52/52 [01:36<00:00,  1.86s/it]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.67s/it]\n",
      "US: 100%|██████████| 52/52 [00:30<00:00,  1.68it/s]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-7b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]\n",
      "US: 100%|██████████| 52/52 [00:41<00:00,  1.24it/s]                 \n"
     ]
    }
   ],
   "source": [
    "for model_id in instruct_models:\n",
    "    del model; print(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        use_safetensors=True,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )   \n",
    "        \n",
    "    pbar = tqdm(states + [\"US\"])\n",
    "    results = {}\n",
    "\n",
    "    for state in pbar:\n",
    "        pbar.set_description(state)\n",
    "        results[state] = {}\n",
    "        \n",
    "        if state == \"US\":\n",
    "            context = message.remove_residency()\n",
    "        else:\n",
    "            context = message.format(persona=state)\n",
    "        \n",
    "        for choice in choices:\n",
    "            cont = \" \" + choice\n",
    "            negative_log_likelihood = continuation_loss(model=model,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        context=context,\n",
    "                                                        cont=cont\n",
    "                                                        )\n",
    "            results[state][choice] = negative_log_likelihood\n",
    "\n",
    "\n",
    "    nll_df = get_nll_df(results=results, num_conts=num_conts) \n",
    "    prob_df, norm_prob_df = get_prob_df(nll_df=nll_df)\n",
    "    diff = get_differences(norm_prob_df=norm_prob_df, columns=columns)\n",
    "    \n",
    "    outputs = {\n",
    "        # \"Negative Log Likelihood\": nll_df.droplevel(0, axis=1),\n",
    "        # \"Probabilities\": prob_df.droplevel(0, axis=1),\n",
    "        \"Normalized Probabilities\": norm_prob_df.droplevel(0, axis=1),\n",
    "        \"Probability Differences\": diff\n",
    "    }\n",
    "\n",
    "    outputs = pd.concat(objs=outputs.values(), keys=outputs.keys(), axis=1)\n",
    "\n",
    "    stats_20 = get_voting_stats(label=\"2020\",\n",
    "                                voting_path=\"../data/voting-2020.xlsx\",\n",
    "                                diff=diff,\n",
    "                                norm_prob_df=norm_prob_df,\n",
    "                                columns=columns)\n",
    "\n",
    "    stats_24 = get_voting_stats(label=\"2024\",\n",
    "                                voting_path=\"../data/voting-2024.xlsx\",\n",
    "                                diff=diff,\n",
    "                                norm_prob_df=norm_prob_df,\n",
    "                                columns=columns)\n",
    "\n",
    "        \n",
    "    f_name = f\"{setting_id}. {os.path.basename(model_id).lower()}\"\n",
    "    with pd.ExcelWriter(os.path.join(\"..\", \"results\", f\"{f_name}.xlsx\")) as writer:\n",
    "        outputs.to_excel(writer, sheet_name=\"Outputs\")  \n",
    "        stats_20.to_excel(writer, sheet_name=\"2020\")\n",
    "        stats_24.to_excel(writer, sheet_name=\"2024\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Agreement\", \"Probability Absolute Difference\", \"Relative Error (winning party)\"]\n",
    "\n",
    "file_names = [f for f in os.listdir(RESULTS_PATH) if not f.startswith(\".\")]\n",
    "summary = {}\n",
    "for f_name in file_names:\n",
    "    df = pd.read_excel(os.path.join(RESULTS_PATH, f_name), index_col=0, header=[0,1], sheet_name=\"Outputs\")\n",
    "    stats_2020 = pd.read_excel(os.path.join(RESULTS_PATH, f_name), index_col=0, header=[0,1], sheet_name=\"2020\")\n",
    "    stats_2024 = pd.read_excel(os.path.join(RESULTS_PATH, f_name), index_col=0, header=[0,1], sheet_name=\"2024\")\n",
    "    \n",
    "    pred_mask = (df[\"Probability Differences\"].drop(\"US\") > 0)\n",
    "    agree_mask = stats_2020.drop(\"US\")[\"Agreement\"].astype(\"bool\")\n",
    "    \n",
    "    all_counts = pred_mask.apply(lambda x: x.value_counts()).fillna(0).astype(int)\n",
    "    all_counts.rename({True:\"Democratic\", False:\"Republican\"}, inplace=True)\n",
    "    all_counts = all_counts.stack().astype(str)\n",
    "\n",
    "    blue_tp_counts = (pred_mask & agree_mask).apply(lambda x: x.value_counts()).fillna(0).astype(int)\n",
    "    red_tp_counts = (~pred_mask & agree_mask).apply(lambda x: x.value_counts()).fillna(0).astype(int)\n",
    "\n",
    "    if \"Democratic\" in all_counts.index:\n",
    "        all_counts[\"Democratic\"] = [f\"{tp}/{k}\" for k, tp in zip(all_counts[\"Democratic\"], blue_tp_counts.loc[True])]\n",
    "    if \"Republican\" in all_counts.index:\n",
    "        all_counts[\"Republican\"] = [f\"{tp}/{k}\" for k, tp in zip(all_counts[\"Republican\"], red_tp_counts.loc[True])]\n",
    "    \n",
    "    \n",
    "    _, base_name = f_name.split(\".\", 1)\n",
    "    base_name, _ = base_name.rsplit(\".\", 1)\n",
    "    summary[base_name.strip()] = pd.concat((all_counts, stats_2020.loc[\" \", cols]))\n",
    "    \n",
    "\n",
    "summary = pd.concat(summary.values(), keys=summary.keys()).to_frame().unstack([1, 2]).sort_index().droplevel(0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(RESULTS_PATH, \"3. mistral-7b-v0.3.xlsx\"), index_col=0, header=[0,1], sheet_name=\"Outputs\")\n",
    "stats_2020 = pd.read_excel(os.path.join(RESULTS_PATH, \"3. mistral-7b-v0.3.xlsx\"), index_col=0, header=[0,1], sheet_name=\"2020\")\n",
    "stats_2024 = pd.read_excel(os.path.join(RESULTS_PATH, \"3. mistral-7b-v0.3.xlsx\"), index_col=0, header=[0,1], sheet_name=\"2024\")\n",
    "\n",
    "pred_mask = (df[\"Probability Differences\"].drop(\"US\") > 0)\n",
    "agree_mask = stats_2020.drop(\"US\")[\"Agreement\"].astype(\"bool\")\n",
    "\n",
    "all_counts = pred_mask.apply(lambda x: x.value_counts()).fillna(0).astype(int)\n",
    "display(all_counts)\n",
    "all_counts.rename({True:\"Democratic\", False:\"Republican\"}, inplace=True)\n",
    "all_counts = all_counts.stack().astype(str)\n",
    "\n",
    "blue_tp_counts = (pred_mask & agree_mask).apply(lambda x: x.value_counts()).fillna(0).astype(int)\n",
    "display(blue_tp_counts)\n",
    "red_tp_counts = (~pred_mask & agree_mask).apply(lambda x: x.value_counts()).fillna(0).astype(int)\n",
    "display(red_tp_counts)\n",
    "\n",
    "\n",
    "if \"Democratic\" in all_counts.index:\n",
    "    all_counts[\"Democratic\"] = [f\"{tp}/{k}\" for k, tp in zip(all_counts[\"Democratic\"], blue_tp_counts.loc[True])]\n",
    "if \"Republican\" in all_counts.index:\n",
    "    all_counts[\"Republican\"] = [f\"{tp}/{k}\" for k, tp in zip(all_counts[\"Republican\"], red_tp_counts.loc[True])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cols = [\"Democratic\", \"Republican\", \"Agreement\", \"Probability Absolute Difference\", \"Relative Error (winning party)\"]\n",
    "summary = summary[ord_cols]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_2020_styled = (\n",
    "    summary.style\n",
    "    .apply(lambda x: _bold_fn(x, fn=max),\n",
    "           subset=\"Agreement\",\n",
    "           axis=1\n",
    "           )\n",
    "    .apply(lambda x: _bold_fn(x, fn=min),\n",
    "           subset=\"Probability Absolute Difference\",\n",
    "           axis=1\n",
    "           )\n",
    "    .apply(lambda x: _bold_fn(x, fn=min),\n",
    "           subset=\"Relative Error (winning party)\",\n",
    "           axis=1\n",
    "           )\n",
    "    .map(lambda _: \"background-color: #d5a6bd\", subset=\"Agreement\")\n",
    "    .map(lambda _: \"background-color: #ffe599\", subset=\"Probability Absolute Difference\")\n",
    "    .map(lambda _: \"background-color: #6fa8dc\", subset=\"Relative Error (winning party)\")\n",
    "    .map(lambda _: \"color: black\")\n",
    "    )\n",
    "\n",
    "display(summary_2020_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elections",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
