{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch_karanikolopoulos/miniconda3/envs/elections/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectionMessage():\n",
    "    \n",
    "    def __init__(self, chat: Union[str, List]) -> None:\n",
    "        self.chat = chat\n",
    "    \n",
    "    def format(self, persona) -> Union[str, List]:\n",
    "        \n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat.format(persona=persona)\n",
    "        \n",
    "        elif isinstance(self.chat, List):\n",
    "            chat = [dict(message) for message in self.chat]\n",
    "            for message in chat:\n",
    "                if \"{persona}\" in message[\"content\"]:\n",
    "                    message[\"content\"] = message[\"content\"].format(persona=persona)\n",
    "            return chat\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat\n",
    "        elif isinstance(self.chat, List):\n",
    "            \n",
    "            r = (f\"User: {self.chat[0]['content']}\",\n",
    "                 f\"Assistant: {self.chat[1]['content']}\")\n",
    "            \n",
    "            return \"\\n\".join(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer:AutoTokenizer, message: Union[str, List]) -> torch.Tensor:\n",
    "    if isinstance(message, str): # let the tokenizer decide for special tokens\n",
    "        return tokenizer(message, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    elif isinstance(message, List):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            conversation=message,\n",
    "            continue_final_message=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "def continuation_loss(\n",
    "    model:AutoModelForCausalLM,\n",
    "    tokenizer:AutoTokenizer,\n",
    "    context: Union[str, List],\n",
    "    cont:str\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    context_encodings = tokenize(tokenizer=tokenizer, message=context)\n",
    "    cont_encodings = tokenizer.encode(cont, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = torch.cat((context_encodings, cont_encodings), dim=1).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits.permute(0, 2, 1) # vocab dimension last\n",
    "    logits = logits[:, :, :-1]\n",
    "\n",
    "    input_ids[:, :-cont_encodings.size(1)] = -100 # makes context ignored by loss function\n",
    "    input_ids = input_ids[:, 1:] # next-token-prediction => shift tokens\n",
    "\n",
    "    \n",
    "    nll_losses = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, input_ids)\n",
    "    \n",
    "    return nll_losses.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DemographicExperiment():\n",
    "    \n",
    "    choices = [\n",
    "        \"Democratic party\",\n",
    "        \"Democratic candidate\",\n",
    "        \"Democratic nominee\",\n",
    "        \"Republican party\",\n",
    "        \"Republican candidate\",\n",
    "        \"Republican nominee\"  \n",
    "    ]\n",
    "    \n",
    "    \n",
    "    gender = {\n",
    "        \"text\": \"As a {persona} U.S. citizen, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"male\",\n",
    "            \"female\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    sex_orient = {\n",
    "        \"text\": \"As a {persona} U.S. citizen, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"straight person\",\n",
    "            \"member of the LGBTQ+ community\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    age = {\n",
    "        \"text\": \"As a U.S. citizen in the {persona} age group, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"18-29\",\n",
    "            \"30-44\",\n",
    "            \"45-64\",\n",
    "            \"65+\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    education = {\n",
    "        \"text\": \"As a U.S. citizen {persona} higher education (college degree or higher), in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"with\",\n",
    "            \"without\"\n",
    "        ]\n",
    "    }\n",
    "     \n",
    "    income = {\n",
    "        \"text\": \"As a U.S. citizen, with income {persona}, in the 2024 U.S presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"below $50,000\",\n",
    "            \"between $50,000 and $100,000\",\n",
    "            \"above $100,000\"\n",
    "        ]\n",
    "    }\n",
    "      \n",
    "    location = {\n",
    "        \"text\": \"As a U.S. citizen, living in {persona} area, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"an urban\",\n",
    "            \"a sub-urban\",\n",
    "            \"a rural\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    religion = {\n",
    "        \"text\": \"As a {persona} U.S. citizen, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"christian\",\n",
    "            \"muslim\",\n",
    "            \"hindu\",\n",
    "            \"jew\",\n",
    "            \"buddhist\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    ethnicity = {\n",
    "        \"text\": \"As a U.S. citizen, living in {persona} area, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"White American\",\n",
    "            \"African American\",\n",
    "            \"European\",\n",
    "            \"Hispanic\",\n",
    "            \"Mexican\",\n",
    "            \"Asian\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    border_ethnicity = {\n",
    "        \"text\": \"As a U.S. citizen of {persona} of White American origin, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"California\",\n",
    "            \"Arizona\",\n",
    "            \"New Mexico\",\n",
    "            \"Texas\",\n",
    "            \"Luisiana\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"gender\": gender,\n",
    "        \"sex_orient\": sex_orient,\n",
    "        \"age\": age,\n",
    "        \"education\": education,\n",
    "        \"income\": income,\n",
    "        \"location\": location,\n",
    "        \"religion\": religion,\n",
    "        \"ethnicity\": ethnicity,\n",
    "        \"border_ethnicity\": border_ethnicity         \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_df(results: Dict, num_conts: int) -> pd.DataFrame:\n",
    "    results = {dem:pd.DataFrame.from_dict(result, orient=\"index\") for dem, result in results.items()}\n",
    "    \n",
    "    nll_df = pd.concat(objs=results.values(), keys=results.keys())\n",
    "    blue_idx = nll_df.iloc[:, :num_conts].columns\n",
    "    red_idx = nll_df.iloc[:, num_conts:].columns\n",
    "\n",
    "    objs = (nll_df[blue_idx], nll_df[red_idx])\n",
    "    nll_df = pd.concat(objs=objs, keys=(\"Democratic\", \"Republican\"), axis=1)\n",
    "    \n",
    "    return nll_df\n",
    "    \n",
    "def get_prob_df(nll_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Probabilities and normalized probabilities for every continuation\"\"\"\n",
    "    \n",
    "    prob_df = (-nll_df).map(lambda x: exp(x)) # exp(LogLikelihood)\n",
    "    # democratic probability sum\n",
    "    prob_df[\"Democratic\", \"D_sum\"] = prob_df[\"Democratic\"].sum(axis=1)\n",
    "    # republican probability sum\n",
    "    prob_df[\"Republican\", \"R_sum\"] = prob_df[\"Republican\"].sum(axis=1)\n",
    "    prob_df = prob_df[[\"Democratic\", \"Republican\"]]\n",
    "    \n",
    "    no_cols = int(len(prob_df.columns) / 2)\n",
    "    norm_prob_df = prob_df.copy()\n",
    "    for i in range(no_cols):   \n",
    "        probs = prob_df.iloc[:, [i, no_cols+i]]\n",
    "        # P(D) / sum(P(D) + P(R))\n",
    "        norm_prob_df.iloc[:, i] = norm_prob_df.iloc[:, i].div(probs.sum(axis=1))\n",
    "        # P(R) / sum(P(D) + P(R))\n",
    "        norm_prob_df.iloc[:, no_cols+i] = norm_prob_df.iloc[:, no_cols+i].div(probs.sum(axis=1))\n",
    "        \n",
    "    return prob_df, norm_prob_df\n",
    "\n",
    "def dump_results(df:pd.DataFrame, name:str) -> None:\n",
    "    df.style.background_gradient(\n",
    "        cmap=\"Greens\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        text_color_threshold=0.3,\n",
    "    ).to_excel(f\"{name}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "border_ethnicity: 100%|██████████| 9/9 [00:16<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "num_conts = int (len(DemographicExperiment.choices) / 2)\n",
    "\n",
    "pbar = tqdm(DemographicExperiment.data.items())\n",
    "for demographic, data in pbar:\n",
    "    pbar.set_description(demographic)\n",
    "    results[demographic] = {}\n",
    "    for option in data[\"options\"]:\n",
    "        results[demographic][option] = {}        \n",
    "        context = data[\"text\"].format(persona=option)        \n",
    "        \n",
    "        for choice in DemographicExperiment.choices:\n",
    "            cont = \" \" + choice\n",
    "            negative_log_likelihood = continuation_loss(model=model,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        context=context,\n",
    "                                                        cont=cont\n",
    "                                                        )\n",
    "        \n",
    "            results[demographic][option][choice] = negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_df = get_nll_df(results=results, num_conts=num_conts)\n",
    "_, norm_prob_df = get_prob_df(nll_df=nll_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_prob_df.style.background_gradient(\n",
    "        cmap=\"Greens\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        text_color_threshold=0.3,\n",
    "    ).to_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elections",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
