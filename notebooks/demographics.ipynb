{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from src.definitions import RESULTS_PATH\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectionMessage():\n",
    "    \n",
    "    def __init__(self, chat: Union[str, List]) -> None:\n",
    "        self.chat = chat\n",
    "    \n",
    "    def format(self, persona) -> Union[str, List]:\n",
    "        \n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat.format(persona=persona)\n",
    "        \n",
    "        elif isinstance(self.chat, List):\n",
    "            chat = [dict(message) for message in self.chat]\n",
    "            for message in chat:\n",
    "                if \"{persona}\" in message[\"content\"]:\n",
    "                    message[\"content\"] = message[\"content\"].format(persona=persona)\n",
    "            return chat\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if isinstance(self.chat, str):\n",
    "            return self.chat\n",
    "        elif isinstance(self.chat, List):\n",
    "            \n",
    "            r = (f\"User: {self.chat[0]['content']}\",\n",
    "                 f\"Assistant: {self.chat[1]['content']}\")\n",
    "            \n",
    "            return \"\\n\".join(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer:AutoTokenizer, message: Union[str, List]) -> torch.Tensor:\n",
    "    if isinstance(message, str): # let the tokenizer decide for special tokens\n",
    "        return tokenizer(message, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    elif isinstance(message, List):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            conversation=message,\n",
    "            continue_final_message=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "def continuation_loss(\n",
    "    model:AutoModelForCausalLM,\n",
    "    tokenizer:AutoTokenizer,\n",
    "    context: Union[str, List],\n",
    "    cont:str\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    context_encodings = tokenize(tokenizer=tokenizer, message=context)\n",
    "    cont_encodings = tokenizer.encode(cont, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = torch.cat((context_encodings, cont_encodings), dim=1).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits.permute(0, 2, 1) # vocab dimension last\n",
    "    logits = logits[:, :, :-1]\n",
    "\n",
    "    input_ids[:, :-cont_encodings.size(1)] = -100 # makes context ignored by loss function\n",
    "    input_ids = input_ids[:, 1:] # next-token-prediction => shift tokens\n",
    "\n",
    "    \n",
    "    nll_losses = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, input_ids)\n",
    "    \n",
    "    return nll_losses.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DemographicExperiment():\n",
    "    \n",
    "    choices = [\n",
    "        \"Democratic party\",\n",
    "        \"Democratic candidate\",\n",
    "        \"Democratic nominee\",\n",
    "        \"Republican party\",\n",
    "        \"Republican candidate\",\n",
    "        \"Republican nominee\"  \n",
    "    ]\n",
    "    \n",
    "    \n",
    "    gender = {\n",
    "        \"text\": \"As a {persona} U.S. citizen, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"male\",\n",
    "            \"female\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    sex_orient = {\n",
    "        \"text\": \"As a {persona} U.S. citizen, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"straight person\",\n",
    "            \"member of the LGBTQ+ community\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    age = {\n",
    "        \"text\": \"As a U.S. citizen in the {persona} age group, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"18-29\",\n",
    "            \"30-44\",\n",
    "            \"45-64\",\n",
    "            \"65+\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    education = {\n",
    "        \"text\": \"As a U.S. citizen {persona} higher education (college degree or higher), in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"with\",\n",
    "            \"without\"\n",
    "        ]\n",
    "    }\n",
    "     \n",
    "    income = {\n",
    "        \"text\": \"As a U.S. citizen, with income {persona}, in the 2024 U.S presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"below $50,000\",\n",
    "            \"between $50,000 and $100,000\",\n",
    "            \"above $100,000\"\n",
    "        ]\n",
    "    }\n",
    "      \n",
    "    location = {\n",
    "        \"text\": \"As a U.S. citizen, living in {persona} area, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"an urban\",\n",
    "            \"a sub-urban\",\n",
    "            \"a rural\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    religion = {\n",
    "        \"text\": \"As a {persona} U.S. citizen, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"christian\",\n",
    "            \"muslim\",\n",
    "            \"hindu\",\n",
    "            \"jew\",\n",
    "            \"buddhist\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    ethnicity = {\n",
    "        \"text\": \"As a U.S. citizen, living in {persona} area, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"White American\",\n",
    "            \"African American\",\n",
    "            \"European\",\n",
    "            \"Hispanic\",\n",
    "            \"Mexican\",\n",
    "            \"Asian\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    border_ethnicity = {\n",
    "        \"text\": \"As a U.S. citizen of {persona} of White American origin, in the 2024 U.S. presidential election I will vote for the\",\n",
    "        \"options\": [\n",
    "            \"California\",\n",
    "            \"Arizona\",\n",
    "            \"New Mexico\",\n",
    "            \"Texas\",\n",
    "            \"Luisiana\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"gender\": gender,\n",
    "        \"sex_orient\": sex_orient,\n",
    "        \"age\": age,\n",
    "        \"education\": education,\n",
    "        \"income\": income,\n",
    "        \"location\": location,\n",
    "        \"religion\": religion,\n",
    "        \"ethnicity\": ethnicity,\n",
    "        \"border_ethnicity\": border_ethnicity         \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_df(results: Dict, num_conts: int) -> pd.DataFrame:\n",
    "    results = {dem:pd.DataFrame.from_dict(result, orient=\"index\") for dem, result in results.items()}\n",
    "    \n",
    "    nll_df = pd.concat(objs=results.values(), keys=results.keys())\n",
    "    blue_idx = nll_df.iloc[:, :num_conts].columns\n",
    "    red_idx = nll_df.iloc[:, num_conts:].columns\n",
    "\n",
    "    objs = (nll_df[blue_idx], nll_df[red_idx])\n",
    "    nll_df = pd.concat(objs=objs, keys=(\"Democratic\", \"Republican\"), axis=1)\n",
    "    \n",
    "    return nll_df\n",
    "    \n",
    "def get_prob_df(nll_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Probabilities and normalized probabilities for every continuation\"\"\"\n",
    "    \n",
    "    prob_df = (-nll_df).map(lambda x: exp(x)) # exp(LogLikelihood)\n",
    "    # democratic probability sum\n",
    "    prob_df[\"Democratic\", \"D_sum\"] = prob_df[\"Democratic\"].sum(axis=1)\n",
    "    # republican probability sum\n",
    "    prob_df[\"Republican\", \"R_sum\"] = prob_df[\"Republican\"].sum(axis=1)\n",
    "    prob_df = prob_df[[\"Democratic\", \"Republican\"]]\n",
    "    \n",
    "    no_cols = int(len(prob_df.columns) / 2)\n",
    "    norm_prob_df = prob_df.copy()\n",
    "    for i in range(no_cols):   \n",
    "        probs = prob_df.iloc[:, [i, no_cols+i]]\n",
    "        # P(D) / sum(P(D) + P(R))\n",
    "        norm_prob_df.iloc[:, i] = norm_prob_df.iloc[:, i].div(probs.sum(axis=1))\n",
    "        # P(R) / sum(P(D) + P(R))\n",
    "        norm_prob_df.iloc[:, no_cols+i] = norm_prob_df.iloc[:, no_cols+i].div(probs.sum(axis=1))\n",
    "        \n",
    "    return prob_df, norm_prob_df\n",
    "\n",
    "def dump_results(df:pd.DataFrame, name:str) -> None:\n",
    "    \n",
    "    df.style.background_gradient(\n",
    "        cmap=\"Greens\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        text_color_threshold=0.3,\n",
    "    ).to_excel(os.path.join(RESULTS_PATH, f\"{name}.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# num_conts = int (len(DemographicExperiment.choices) / 2)\n",
    "\n",
    "# pbar = tqdm(DemographicExperiment.data.items())\n",
    "# for demographic, data in pbar:\n",
    "#     pbar.set_description(demographic)\n",
    "#     results[demographic] = {}\n",
    "#     for option in data[\"options\"]:\n",
    "#         results[demographic][option] = {}        \n",
    "#         context = data[\"text\"].format(persona=option)        \n",
    "        \n",
    "#         for choice in DemographicExperiment.choices:\n",
    "#             cont = \" \" + choice\n",
    "#             negative_log_likelihood = continuation_loss(model=model,\n",
    "#                                                         tokenizer=tokenizer,\n",
    "#                                                         context=context,\n",
    "#                                                         cont=cont\n",
    "#                                                         )\n",
    "        \n",
    "#             results[demographic][option][choice] = negative_log_likelihood\n",
    "\n",
    "# nll_df = get_nll_df(results=results, num_conts=num_conts)\n",
    "# _, norm_prob_df = get_prob_df(nll_df=nll_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "base_models = [\n",
    "\"meta-llama/Llama-3.1-8B\",\n",
    "\"meta-llama/Llama-3.2-3B\",\n",
    "\"google/gemma-2-9b\",\n",
    "\"mistralai/Mistral-7B-v0.3\",\n",
    "\"tiiuae/falcon-7b\"\n",
    "]\n",
    "\n",
    "instruct_models = [\n",
    "\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "\"google/gemma-2-9b-it\",\n",
    "\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "\"tiiuae/falcon-7b-instruct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.1-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.09s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:16<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:10<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2-9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.21it/s]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:31<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.58s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:16<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:14<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:16<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:10<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2-9b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:31<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.55s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:16<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-7b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.87s/it]\n",
      "border_ethnicity: 100%|██████████| 9/9 [00:14<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "for model_id in base_models + instruct_models:\n",
    "    print(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        use_safetensors=True,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    num_conts = int (len(DemographicExperiment.choices) / 2)\n",
    "\n",
    "    pbar = tqdm(DemographicExperiment.data.items())\n",
    "    for demographic, data in pbar:\n",
    "        pbar.set_description(demographic)\n",
    "        results[demographic] = {}\n",
    "        for option in data[\"options\"]:\n",
    "            results[demographic][option] = {}        \n",
    "            context = data[\"text\"].format(persona=option)        \n",
    "            \n",
    "            for choice in DemographicExperiment.choices:\n",
    "                cont = \" \" + choice\n",
    "                negative_log_likelihood = continuation_loss(model=model,\n",
    "                                                            tokenizer=tokenizer,\n",
    "                                                            context=context,\n",
    "                                                            cont=cont\n",
    "                                                            )\n",
    "            \n",
    "                results[demographic][option][choice] = negative_log_likelihood\n",
    "            \n",
    "            \n",
    "    nll_df = get_nll_df(results=results, num_conts=num_conts)\n",
    "    _, norm_prob_df = get_prob_df(nll_df=nll_df)\n",
    "    \n",
    "    f_name = os.path.basename(model_id).lower()\n",
    "    dump_results(df=norm_prob_df, name=f_name)\n",
    "    \n",
    "    del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elections",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
